{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "edr4tgy7ytyscr3zsy6l",
   "authorId": "4503098686890",
   "authorName": "AMELATTI",
   "authorEmail": "anthony.melatti@snowflake.com",
   "sessionId": "86f7afc4-a90b-488f-8547-800feb85731d",
   "lastEditTime": 1755896606248
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nsession = get_active_session()\n\n# Add a query tag to the session. This helps with debugging and performance monitoring.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"cr_notebooks_resolution\", \n                     \"version\":{\"major\":1, \"minor\":0},\n                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n\n# Set session context \nsession.use_role(\"EMBEDDING_MODEL_HOL_USER\") \nsession.use_database(\"EMBEDDING_MODEL_HOL_DB\")\nsession.use_schema(\"EMBEDDING_MODEL_HOL_SCHEMA\")\n\n# Print the current role, warehouse, and database/schema\nprint(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")\n     ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a6ee59e9-9d12-4dc1-a148-1511445399c7",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "! pip install transformers --quiet\n! pip install torch --quiet",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# This cell generates random examples to use at the end of the notebook for testing\n\nimport random\nimport pandas as pd\n\n# Define 20 clean tech companies\nclean_tech_companies = [\n    \"Google\", \"Microsoft\", \"Apple\", \"Amazon\", \"Meta\",\n    \"Netflix\", \"Nvidia\", \"Intel\", \"Oracle\", \"Salesforce\",\n    \"Adobe\", \"Dropbox\", \"Airbnb\", \"Uber\", \"Lyft\",\n    \"Palantir\", \"Snowflake\", \"Stripe\", \"Zoom\", \"Spotify\"\n]\n\n# Sample engineering job titles to inject as noise\nengineering_titles = [\n    \"Software Engineer\", \"Backend Engineer\", \"Frontend Developer\",\n    \"DevOps Engineer\", \"ML Engineer\", \"Data Engineer\",\n    \"SRE\", \"Embedded Systems Engineer\", \"Security Engineer\",\n    \"Principal Engineer\", \"Engineering Manager\"\n]\n\n# Colleges for added noise\ncolleges = [\"MIT\", \"Stanford\", \"Berkeley\", \"CMU\", \"Harvard\", \"Waterloo\", \"Georgia Tech\"]\n\n# Function to generate messy variants for a tech company\ndef generate_messy_variants_tech(name):\n    variants = [\n        name.lower(),\n        name.upper(),\n        name + \" Inc.\",\n        name + \" LLC\",\n        f\"{name} Technologies\",\n        f\"{name}.com\",\n        f\"Worked at {name}\",\n        f\"{name} - Engineering\",\n        f\"{name} (Remote)\",\n        f\"{name} | USA\",\n        f\"{name} Corp\",\n        f\"{name} - {random.choice(engineering_titles)}\",\n    ]\n    return random.sample(variants, k=min(5, len(variants)))\n\n# Function to inject engineering or college noise\ndef inject_tech_noise(text):\n    noise_options = [\n        f\"{text} - {random.choice(engineering_titles)}\",\n        f\"{random.choice(colleges)} alum, {text}\",\n        f\"{text}, {random.choice(colleges)}\",\n        f\"{text} | {random.choice(engineering_titles)}\",\n        f\"Ex-{text} engineer\"\n    ]\n    return random.choice(noise_options)\n\n# Function to introduce typos into a word\ndef introduce_typos(text, max_typos=2):\n    text = list(text)\n    n_typos = random.randint(1, max_typos)\n    for _ in range(n_typos):\n        if len(text) == 0:\n            break\n        idx = random.randint(0, len(text) - 1)\n        typo_type = random.choice([\"delete\", \"swap\", \"replace\"])\n        if typo_type == \"delete\":\n            del text[idx]\n        elif typo_type == \"swap\" and idx < len(text) - 1:\n            text[idx], text[idx + 1] = text[idx + 1], text[idx]\n        elif typo_type == \"replace\":\n            text[idx] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n    return ''.join(text)\n\n# Generate synthetic tech dataset\nsynthetic_tech_data = []\nfor company in clean_tech_companies:\n    for variant in generate_messy_variants_tech(company):\n        synthetic_tech_data.append({\n            \"messy_company_name\": variant,\n            \"canonical_company_name\": company\n        })\n\n# Sample 100 messy entries\nsynthetic_df = pd.DataFrame(synthetic_tech_data)\nsynthetic_df = synthetic_df.sample(n=100, replace=True, random_state=123).reset_index(drop=True)\n\n# Add noise and typos\nnoisy_tech_names = []\nfor messy_name in synthetic_df[\"messy_company_name\"]:\n    if random.random() < 0.25:\n        noisy_tech_names.append(inject_tech_noise(messy_name))\n    elif (random.random() > 0.25) and (random.random() < 0.6):\n        noisy_tech_names.append(introduce_typos(messy_name))\n    else:\n        noisy_tech_names.append(messy_name)\n\nsynthetic_df[\"messy_company_name\"] = noisy_tech_names",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "561eeb24-1bc6-43b8-991e-8030e9f0cac7",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "synthetic_snowpark_df = session.create_dataframe(synthetic_df)\nsynthetic_snowpark_df.write.mode(\"overwrite\").save_as_table(\"candidate_database\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29998d6a-41e0-4228-9295-b8685ae59d84",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "synthetic_snowpark_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d671f9d4-8e27-4605-b794-4aef2e91ae9b",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Model Import - Run in notebook to test"
  },
  {
   "cell_type": "code",
   "id": "4204580a-fcfc-497f-b379-af3e61e7cb23",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "torch.cuda.is_available()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a9c8fc8-5f5d-4727-a928-33332cc80732",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nmodel.to(device)\n\npremise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\nhypothesis = \"Emmanuel Macron is the President of France\"\n\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(device)\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9da2f8dd-af22-4bd7-9541-37d5d8e7e295",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "input",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d702b47b-1762-489e-bd94-d0abbd8198e8",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "import transformers\nfrom snowflake.ml.registry import Registry\n\n# Create Model Registry\nreg = Registry(\n    session=session, \n    database_name=session.get_current_database(), \n    schema_name=session.get_current_schema()\n    )\n\n# 1. Create the Hugging Face pipeline with softmax activation\nres_pipeline = transformers.pipeline(\n    task='text-classification',\n    model='MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n    tokenizer='MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n    function_to_apply='softmax',\n    return_all_scores=True, \n    device=\"cuda:0\",     # or -1 for CPU\n    top_k=6,\n    batch_size=3,\n    torch_dtype='float16'    \n)\n\nmv = reg.log_model(\n    res_pipeline,\n    model_name=\"MoritzLaurer_DeBERTa_nli_v1\",\n    version_name=\"v1\",\n    pip_requirements=[\"transformers\", \"torch\", \"pyarrow<19.0.0\"],\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d7d00ee-56c2-4ff9-a21d-cae1082744fd",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "reg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d51cbfd0-8ea0-4370-97e9-2c1191f61b72",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# 5. Deploy the model into Snowpark Container Services (SPCS)\nmv.create_service(\n    service_name='nli_bert_svc',\n    service_compute_pool='GPU_NV_S_COMPUTE_POOL',\n    image_repo=f\"{session.get_current_database()}.{session.get_current_schema()}.MY_INFERENCE_IMAGES\",\n    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\", #allows access to pypi to build\n    ingress_enabled=True,\n    gpu_requests        = \"1\",\n    max_instances       = 2\n)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "970992c3-6bf0-43b7-ac4e-4a6005170543",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "-- SHOW SERVICES;\nALTER SERVICE NLI_BERT_SVC SUSPEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f2920cd-f269-4d8a-b313-91598d518d82",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": "SHOW ENDPOINTS IN SERVICE NLI_BERT_SVC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1486593-f0bc-41a9-a480-4f7b9728536a",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "from pprint import pprint\n\nsample_inputs = [\n    \"I love this game, it's amazing!, I am having a great time playing this game\",\n    \"This is the best game ever, absolutely the worst game I ever played\",\n    \"pizza is the best food ever, I can't stand this video game\"\n]\n\ntest_df = pd.DataFrame({ 'text': sample_inputs})\nresults = mv.run(\n    test_df,\n    service_name='nli_bert_svc'\n)\npprint(results['labels'].tolist())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e77dec3c-a204-4c5e-861f-ff74a2b25426",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "synthetic_df['text'] = synthetic_df['messy_company_name'] + ', ' + synthetic_df['canonical_company_name']\nprint(synthetic_df.shape)\nsynthetic_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4061670f-6ac3-4c01-8220-d123228799d6",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "from pprint import pprint\n\ntest_df = synthetic_df[['text']]\nresults = mv.run(\n    test_df,\n    service_name='nli_bert_svc'\n)\npprint(results['labels'].tolist())",
   "execution_count": null
  }
 ]
}